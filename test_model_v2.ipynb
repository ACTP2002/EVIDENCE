{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPd5g1SmSwWTfB+Q+LvkCTo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ACTP2002/EVIDENCE/blob/behavior_model/test_model_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --no-cache-dir -U numpy pandas scipy scikit-learn joblib tensorflow shap h2o"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rv-ePq2q2I_l",
        "outputId": "a1d9b86d-2a8a-407f-fe71-6e11dbed284b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Collecting numpy\n",
            "  Downloading numpy-2.4.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (3.0.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (1.17.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.8.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (1.5.3)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.20.0)\n",
            "Requirement already satisfied: shap in /usr/local/lib/python3.12/dist-packages (0.50.0)\n",
            "Requirement already satisfied: h2o in /usr/local/lib/python3.12/dist-packages (3.46.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: threadpoolctl>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.12.19)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.7.0)\n",
            "Requirement already satisfied: google_pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt_einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (26.0)\n",
            "Requirement already satisfied: protobuf>=5.28.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: typing_extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.1.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.76.0)\n",
            "Requirement already satisfied: tensorboard~=2.20.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.20.0)\n",
            "Requirement already satisfied: keras>=3.10.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.15.1)\n",
            "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.12/dist-packages (from shap) (4.67.3)\n",
            "Requirement already satisfied: slicer==0.0.8 in /usr/local/lib/python3.12/dist-packages (from shap) (0.0.8)\n",
            "Requirement already satisfied: numba>=0.54 in /usr/local/lib/python3.12/dist-packages (from shap) (0.60.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.12/dist-packages (from shap) (3.1.2)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.12/dist-packages (from h2o) (0.9.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.46.3)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.10.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.10.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.10.0->tensorflow) (0.18.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.54->shap) (0.43.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2026.1.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.20.0->tensorflow) (3.10.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.20.0->tensorflow) (11.3.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.20.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.20.0->tensorflow) (3.1.5)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.10.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.10.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.stats import median_abs_deviation\n",
        "import random\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "import h2o\n",
        "from h2o.estimators import H2OExtendedIsolationForestEstimator\n",
        "from h2o.estimators import H2OPrincipalComponentAnalysisEstimator\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, callbacks\n",
        "\n",
        "import shap\n",
        "import joblib\n"
      ],
      "metadata": {
        "id": "5pjmOL-a14_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "class BehaviorInferenceEngine:\n",
        "  RISK_MAPPING = {\n",
        "\n",
        "    # Monetary Behavior\n",
        "    \"mod_z_score_abs\": {\n",
        "        \"category\": \"Monetary Deviation\",\n",
        "        \"template\": \"Transaction amount deviates from user's historical behavior.\"\n",
        "    },\n",
        "    \"ewma_resid\": {\n",
        "        \"category\": \"Monetary Deviation\",\n",
        "        \"template\": \"Transaction differs from recent spending trend.\"\n",
        "    },\n",
        "    \"net_flow_1d\": {\n",
        "        \"category\": \"Liquidity Shift\",\n",
        "        \"template\": \"Unusual daily net cash flow movement detected.\"\n",
        "    },\n",
        "\n",
        "    # Temporal Behavior\n",
        "    \"gap_log\": {\n",
        "        \"category\": \"Temporal Anomaly\",\n",
        "        \"template\": \"Transaction timing gap is inconsistent with prior activity.\"\n",
        "    },\n",
        "\n",
        "    # Access Risk\n",
        "    \"login_count_1h\": {\n",
        "        \"category\": \"Access Risk\",\n",
        "        \"template\": \"Abnormal login frequency observed.\"\n",
        "    },\n",
        "    \"failed_login_ratio_1h\": {\n",
        "        \"category\": \"Access Risk\",\n",
        "        \"template\": \"Elevated failed login attempts detected.\"\n",
        "    },\n",
        "    \"new_ip_1d\": {\n",
        "        \"category\": \"Access Risk\",\n",
        "        \"template\": \"Transaction initiated from a new IP address.\"\n",
        "    },\n",
        "\n",
        "    # Geographic Risk\n",
        "    \"is_cross_border\": {\n",
        "        \"category\": \"Geolocation Risk\",\n",
        "        \"template\": \"Transaction occurred outside user's residence country.\"\n",
        "    }\n",
        "}\n",
        "\n",
        "  def __init__(self, model_dir=\"behavior_assets\"):\n",
        "    self.model_dir = model_dir\n",
        "    self.cat_features = [\"currency\", \"channel\", \"event_type\", \"geo_country\"]\n",
        "    self.num_features = [\"mod_z_score_abs\", \"ewma_resid\", \"gap_log\", \"net_flow_1d\", \"login_count_1h\", \"failed_login_ratio_1h\", \"new_ip_1d\", \"is_cross_border\"]\n",
        "    self.lstm_features = [\"amount_abs\", \"gap_log\", \"amount_to_income_ratio\", \"net_flow_1d\", \"deposit_to_income_ratio\", \"mod_z_score_abs\", \"ewma_resid\"]\n",
        "\n",
        "  # HELPER FUNCTIONS FOR SHAP\n",
        "  def _classify_severity(self, score, is_anomaly):\n",
        "      if score < 0.30 or not is_anomaly:\n",
        "          return \"LOW\"\n",
        "      elif score < 0.70:\n",
        "          return \"MEDIUM\"\n",
        "      else:\n",
        "          return \"HIGH\"\n",
        "\n",
        "  def _apply_confidence(self, df, threshold, std):\n",
        "      k = 3.0 / (std + 1e-9) # Using 3.0 makes the curve slightly smoother\n",
        "\n",
        "      # Calculate distance from threshold\n",
        "      # If positive, it's an anomaly; if negative, it's normal\n",
        "      diff = df[\"final_score\"] - threshold\n",
        "\n",
        "      # Apply Sigmoid\n",
        "      # This maps scores:\n",
        "      # Much higher than threshold -> ~1.0\n",
        "      # Exactly threshold -> 0.5\n",
        "      # Much lower than threshold -> ~0.0 (meaning 100% confident it is NORMAL)\n",
        "      conf = 1 / (1 + np.exp(-k * diff))\n",
        "\n",
        "      # Confidence in the decision (0.5 to 1)\n",
        "      return np.where(conf >= 0.5, conf, 1 - conf)\n",
        "\n",
        "  def _generate_human_explanation(self, feature, value, shap_value):\n",
        "\n",
        "      direction = \"increased\" if shap_value > 0 else \"reduced\"\n",
        "\n",
        "      def safe_float(v):\n",
        "          try:\n",
        "              return float(v)\n",
        "          except:\n",
        "              return None\n",
        "\n",
        "      value = safe_float(value)\n",
        "\n",
        "      explanations = {\n",
        "          \"mod_z_score_abs\": (\n",
        "              f\"Transaction amount deviates significantly from user's normal behavior \"\n",
        "              f\"(Z-score={value:.2f}). This {direction} anomaly risk.\"\n",
        "              if value is not None else\n",
        "              f\"Transaction amount deviates significantly from user's normal behavior. \"\n",
        "              f\"This {direction} anomaly risk.\"\n",
        "          ),\n",
        "\n",
        "          \"ewma_resid\": (\n",
        "              f\"Recent transaction amount differs from short-term trend \"\n",
        "              f\"(EWMA residual={value:.2f}). This {direction} anomaly risk.\"\n",
        "              if value is not None else\n",
        "              f\"Recent transaction amount differs from short-term trend. \"\n",
        "              f\"This {direction} anomaly risk.\"\n",
        "          ),\n",
        "\n",
        "          \"gap_log\": (\n",
        "              f\"Transaction timing gap is unusual compared to prior activity \"\n",
        "              f\"(gap_log={value:.2f}). This {direction} anomaly risk.\"\n",
        "              if value is not None else\n",
        "              f\"Transaction timing gap is unusual compared to prior activity. \"\n",
        "              f\"This {direction} anomaly risk.\"\n",
        "          ),\n",
        "\n",
        "          \"net_flow_1d\": (\n",
        "              f\"Daily net cash flow shift detected \"\n",
        "              f\"(net_flow_1d={value:.2f}). This {direction} anomaly risk.\"\n",
        "              if value is not None else\n",
        "              f\"Daily net cash flow shift detected. \"\n",
        "              f\"This {direction} anomaly risk.\"\n",
        "          ),\n",
        "\n",
        "          \"login_count_1h\": (\n",
        "              f\"Abnormal login frequency in past hour \"\n",
        "              f\"(count={int(value)}). This {direction} anomaly risk.\"\n",
        "              if value is not None else\n",
        "              f\"Abnormal login frequency in past hour. \"\n",
        "              f\"This {direction} anomaly risk.\"\n",
        "          ),\n",
        "\n",
        "          \"failed_login_ratio_1h\": (\n",
        "              f\"Elevated failed login attempts ratio \"\n",
        "              f\"({value:.2f}). This {direction} anomaly risk.\"\n",
        "              if value is not None else\n",
        "              f\"Elevated failed login attempts detected. \"\n",
        "              f\"This {direction} anomaly risk.\"\n",
        "          ),\n",
        "\n",
        "          \"new_ip_1d\":\n",
        "              f\"New IP address detected in last 24h. \"\n",
        "              f\"This {direction} anomaly risk.\",\n",
        "\n",
        "          \"is_cross_border\":\n",
        "              f\"Transaction occurred outside user's residence country. \"\n",
        "              f\"This {direction} anomaly risk.\"\n",
        "      }\n",
        "\n",
        "      return explanations.get(\n",
        "          feature,\n",
        "          f\"{feature} contributed to anomaly score and {direction} risk.\"\n",
        "      )\n",
        "\n",
        "  def _compute_shap_batch(self, df_input):\n",
        "\n",
        "      if isinstance(df_input, dict):\n",
        "          df_input = pd.DataFrame([df_input])\n",
        "\n",
        "      h2o_frame = h2o.H2OFrame(df_input)\n",
        "\n",
        "      for col in self.cat_features:\n",
        "          if col in df_input.columns:\n",
        "              h2o_frame[col] = h2o_frame[col].asfactor()\n",
        "\n",
        "      shap_val = self.surrogate.predict_contributions(h2o_frame)\n",
        "      shap_df = shap_val.as_data_frame().drop(columns=[\"BiasTerm\"])\n",
        "\n",
        "      return shap_df\n",
        "\n",
        "  def _convert_to_risk_evidence(self, feature, shap_value, raw_value):\n",
        "\n",
        "      if feature not in self.RISK_MAPPING:\n",
        "          return None\n",
        "\n",
        "      risk_info = self.RISK_MAPPING[feature]\n",
        "      direction = \"increased\" if shap_value > 0 else \"reduced\"\n",
        "\n",
        "      explanation_text = self._generate_human_explanation(\n",
        "          feature, raw_value, shap_value\n",
        "      )\n",
        "\n",
        "      return {\n",
        "          \"risk_category\": risk_info[\"category\"],\n",
        "          \"feature\": feature,\n",
        "          \"impact\": direction,\n",
        "          \"contribution\": float(shap_value),\n",
        "          \"explanation\": explanation_text\n",
        "      }\n",
        "\n",
        "\n",
        "  def _build_behavior_output(self, df_input, top_n=2, min_abs_contribution=1e-4):\n",
        "\n",
        "      if isinstance(df_input, dict):\n",
        "          df_input = pd.DataFrame([df_input])\n",
        "\n",
        "      shap_df = self._compute_shap_batch(df_input)\n",
        "\n",
        "      results = []\n",
        "\n",
        "      for idx in range(len(df_input)):\n",
        "\n",
        "          row = df_input.iloc[idx]\n",
        "          shap_series = shap_df.iloc[idx]\n",
        "\n",
        "          # Remove tiny noise\n",
        "          shap_series = shap_series[shap_series.abs() > min_abs_contribution]\n",
        "\n",
        "          # Sort by absolute contribution\n",
        "          shap_series = shap_series.reindex(\n",
        "              shap_series.abs().sort_values(ascending=False).index\n",
        "          )\n",
        "\n",
        "          # Top drivers\n",
        "          top_features = shap_series.head(top_n)\n",
        "\n",
        "          evidence_list = []\n",
        "\n",
        "          for feature, shap_value in top_features.items():\n",
        "\n",
        "              raw_value = row.get(feature, None)\n",
        "\n",
        "              evidence = self._convert_to_risk_evidence(\n",
        "                  feature, shap_value, raw_value\n",
        "              )\n",
        "\n",
        "              if evidence:\n",
        "                  evidence_list.append(evidence)\n",
        "\n",
        "          # -------- Highest impact signal --------\n",
        "          if evidence_list:\n",
        "              highest_signal = evidence_list[0][\"risk_category\"]\n",
        "          else:\n",
        "              highest_signal = \"No significant anomaly drivers detected.\"\n",
        "\n",
        "          # -------- Severity --------\n",
        "          anomaly_level = float(row[\"final_score\"]/self.threshold)\n",
        "          severity = self._classify_severity(anomaly_level,row[\"is_anomaly\"])\n",
        "\n",
        "          # -------- Output --------\n",
        "\n",
        "          result = {\n",
        "              \"event_time\": str(row[\"event_time\"]),\n",
        "              \"txn_id\": str(row[\"txn_id\"]),\n",
        "              \"user_id\": str(row[\"user_id\"]),\n",
        "              \"is_anomaly\": int(row[\"is_anomaly\"]),\n",
        "              \"detector_type\": \"BEHAVIOR\",\n",
        "              \"signal\": highest_signal,\n",
        "              \"severity\": severity,\n",
        "              \"confidence\": str(row[\"confidence_score\"]),\n",
        "              \"evidence\": evidence_list\n",
        "          }\n",
        "\n",
        "          results.append(result)\n",
        "\n",
        "      if len(results) == 1:\n",
        "          return results[0]\n",
        "\n",
        "      return results\n",
        "\n",
        "  def save_assets(self, eif, surrogate, lstm, seq_scaler, feature_scaler, cohort_stats, global_stats, threshold, weights, scores_std):\n",
        "    if not os.path.exists(self.model_dir): os.makedirs(self.model_dir)\n",
        "    h2o.save_model(model=eif, path=self.model_dir, force=True)\n",
        "    h2o.save_model(model=surrogate, path=self.model_dir, force=True)\n",
        "    lstm.save(f\"{self.model_dir}/lstm_model.h5\")\n",
        "    joblib.dump(seq_scaler, f\"{self.model_dir}/seq_scaler.pkl\")\n",
        "    joblib.dump(feature_scaler, f\"{self.model_dir}/feature_scaler.pkl\")\n",
        "    joblib.dump(cohort_stats, f\"{self.model_dir}/cohort_stats.pkl\")\n",
        "    joblib.dump(global_stats, f\"{self.model_dir}/global_stats.pkl\")\n",
        "    joblib.dump({\"threshold\": threshold, \"weights\": weights, \"eif_id\": eif.model_id, \"surr_id\": surrogate.model_id, \"scores_std\": scores_std}, f\"{self.model_dir}/meta.pkl\")\n",
        "\n",
        "  def load_assets(self):\n",
        "    h2o.init()\n",
        "    meta = joblib.load(f\"{self.model_dir}/meta.pkl\")\n",
        "    self.threshold = meta['threshold']\n",
        "    self.weights = meta['weights']\n",
        "    self.scores_std = meta['scores_std']\n",
        "    self.eif = h2o.load_model(f\"{self.model_dir}/{meta['eif_id']}\")\n",
        "    self.surrogate = h2o.load_model(f\"{self.model_dir}/{meta['surr_id']}\")\n",
        "    self.lstm = tf.keras.models.load_model(f\"{self.model_dir}/lstm_model.h5\", custom_objects={\"mse\": tf.keras.losses.MeanSquaredError()})\n",
        "    self.seq_scaler = joblib.load(f\"{self.model_dir}/seq_scaler.pkl\")\n",
        "    self.feature_scaler = joblib.load(f\"{self.model_dir}/feature_scaler.pkl\")\n",
        "    self.cohort_stats = joblib.load(f\"{self.model_dir}/cohort_stats.pkl\")\n",
        "    self.global_stats = joblib.load(f\"{self.model_dir}/global_stats.pkl\")\n",
        "\n",
        "  def _engineer(self, df):\n",
        "    df = df.copy()\n",
        "    df[\"event_time\"] = pd.to_datetime(df[\"event_time\"])\n",
        "    df = df.sort_values(\"event_time\")\n",
        "    for c in [\"currency\", \"channel\", \"residence_country\", \"geo_country\", \"event_type\"]:\n",
        "        if c in df.columns: df[c] = df[c].astype(str).str.strip().str.lower()\n",
        "\n",
        "    # Financial ratios calculation\n",
        "    df[\"amount_abs\"] = df[\"amount\"].abs()\n",
        "    df[\"amount_to_income_ratio\"] = df[\"amount_abs\"] / (df[\"declared_income\"] + 1e-9)\n",
        "    df[\"deposit_to_income_ratio\"] = df[\"account_deposit\"] / (df[\"declared_income\"] + 1e-9)\n",
        "    df[\"net_flow_1d\"] = df[\"amount_in_1d\"] - df[\"amount_out_1d\"]\n",
        "\n",
        "    # Location\n",
        "    df[\"is_cross_border\"] = (df[\"residence_country\"] != df[\"geo_country\"]).astype(int)\n",
        "    df[\"failed_login_ratio_1h\"] = df[\"failed_login_1h\"] / (df[\"login_count_1h\"] + 1e-9)\n",
        "    df[\"new_ip_1d\"] = df[\"new_ip_1d\"].fillna(0)\n",
        "    df[\"geo_change_1d\"] = df[\"geo_change_1d\"].fillna(0)\n",
        "\n",
        "    # Transaction gap\n",
        "    df[\"gap_seconds\"] = df[\"event_time\"].diff().dt.total_seconds().fillna(self.global_stats['median_gap'])\n",
        "    df[\"gap_log\"] = np.log1p(df[\"gap_seconds\"])\n",
        "\n",
        "    # Window period\n",
        "    df[\"user_median_15\"] = df[\"amount_abs\"].rolling(window=15, min_periods=1).median()\n",
        "    df[\"user_mad_15\"] = df[\"amount_abs\"].rolling(window=15, min_periods=1).apply(lambda x: median_abs_deviation(x, scale='normal') if len(x)>1 else self.global_stats['mad'], raw=False)\n",
        "\n",
        "    for col in [\"cohort_median\", \"cohort_mad\"]:\n",
        "      if col in df.columns:\n",
        "          df = df.drop(columns=col)\n",
        "\n",
        "    df = df.merge(self.cohort_stats, on=[\"currency\", \"geo_country\", \"channel\", \"event_type\"], how=\"left\")\n",
        "    b_med = df[\"user_median_15\"].fillna(df[\"cohort_median\"]).fillna(self.global_stats['median_amt'])\n",
        "    b_mad = df[\"user_mad_15\"].fillna(df[\"cohort_mad\"]).fillna(self.global_stats['mad'])\n",
        "    df[\"mod_z_score_abs\"] = (0.6745 * (df[\"amount_abs\"] - b_med) / (b_mad + 1e-9)).abs()\n",
        "    df[\"ewma_resid\"] = (df[\"amount_abs\"] - df[\"amount_abs\"].ewm(span=8).mean()).abs()\n",
        "    return df\n",
        "\n",
        "  def predict(self, raw_json):\n",
        "    # 1. Parse & Engineer\n",
        "    input_df = pd.DataFrame(json.loads(raw_json))\n",
        "    feat_df = self._engineer(input_df)\n",
        "\n",
        "    # 2. Points Score (EIF)\n",
        "    h2o_fr = h2o.H2OFrame(feat_df[self.cat_features + self.num_features])\n",
        "    for c in self.cat_features: h2o_fr[c] = h2o_fr[c].asfactor()\n",
        "    feat_df[\"iforest_score\"] = self.eif.predict(h2o_fr)[\"anomaly_score\"].as_data_frame().iloc[:, 0].values\n",
        "\n",
        "    # 3. Sequence Score (LSTM)\n",
        "    scaled_lstm = self.seq_scaler.transform(feat_df[self.lstm_features].fillna(0))\n",
        "    if len(scaled_lstm) >= 20:\n",
        "        seq = np.array([scaled_lstm[-20:]])\n",
        "        feat_df.loc[feat_df.index[-1], \"lstm_score\"] = np.mean((seq - self.lstm.predict(seq, verbose=0))**2)\n",
        "    else:\n",
        "        feat_df[\"lstm_score\"] = self.global_stats['lstm_median']\n",
        "\n",
        "    # 4. Ensemble\n",
        "    detector_cols = [\"mod_z_score_abs\", \"ewma_resid\", \"iforest_score\", \"lstm_score\"]\n",
        "    n_vals = self.feature_scaler.transform(feat_df[detector_cols].fillna(0))\n",
        "    feat_df[\"final_score\"] = np.sum([self.weights[c] * n_vals[:, i] for i, c in enumerate(detector_cols)], axis=0)\n",
        "    feat_df[\"is_anomaly\"] = (feat_df[\"final_score\"] >= self.threshold).astype(int)\n",
        "    feat_df[\"confidence_score\"] = self._apply_confidence(feat_df, self.threshold, self.scores_std)\n",
        "\n",
        "    # 5. SHAP & Formatting\n",
        "    final_prediction = self._build_behavior_output(feat_df)\n",
        "\n",
        "    return final_prediction\n"
      ],
      "metadata": {
        "id": "4jxqlI7t1veN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KKOMC5hv1RIW",
        "outputId": "d0302628-a9fb-4fec-e9fc-c61ec4274164"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking whether there is an H2O instance running at http://localhost:54321. connected.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "--------------------------  -----------------------------------------------------------------------------------------\n",
              "H2O_cluster_uptime:         10 mins 32 secs\n",
              "H2O_cluster_timezone:       Etc/UTC\n",
              "H2O_data_parsing_timezone:  UTC\n",
              "H2O_cluster_version:        3.46.0.9\n",
              "H2O_cluster_version_age:    2 months and 19 days\n",
              "H2O_cluster_name:           H2O_from_python_unknownUser_l582pu\n",
              "H2O_cluster_total_nodes:    1\n",
              "H2O_cluster_free_memory:    3.075 Gb\n",
              "H2O_cluster_total_cores:    2\n",
              "H2O_cluster_allowed_cores:  2\n",
              "H2O_cluster_status:         locked, healthy\n",
              "H2O_connection_url:         http://localhost:54321\n",
              "H2O_connection_proxy:       {\"http\": null, \"https\": null, \"colab_language_server\": \"/usr/colab/bin/language_service\"}\n",
              "H2O_internal_security:      False\n",
              "Python_version:             3.12.12 final\n",
              "--------------------------  -----------------------------------------------------------------------------------------"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "\n",
              "#h2o-table-9.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-9 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-9 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-9 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-9 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-9 .h2o-table th,\n",
              "#h2o-table-9 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-9 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-9\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption></caption>\n",
              "    <thead></thead>\n",
              "    <tbody><tr><td>H2O_cluster_uptime:</td>\n",
              "<td>10 mins 32 secs</td></tr>\n",
              "<tr><td>H2O_cluster_timezone:</td>\n",
              "<td>Etc/UTC</td></tr>\n",
              "<tr><td>H2O_data_parsing_timezone:</td>\n",
              "<td>UTC</td></tr>\n",
              "<tr><td>H2O_cluster_version:</td>\n",
              "<td>3.46.0.9</td></tr>\n",
              "<tr><td>H2O_cluster_version_age:</td>\n",
              "<td>2 months and 19 days</td></tr>\n",
              "<tr><td>H2O_cluster_name:</td>\n",
              "<td>H2O_from_python_unknownUser_l582pu</td></tr>\n",
              "<tr><td>H2O_cluster_total_nodes:</td>\n",
              "<td>1</td></tr>\n",
              "<tr><td>H2O_cluster_free_memory:</td>\n",
              "<td>3.075 Gb</td></tr>\n",
              "<tr><td>H2O_cluster_total_cores:</td>\n",
              "<td>2</td></tr>\n",
              "<tr><td>H2O_cluster_allowed_cores:</td>\n",
              "<td>2</td></tr>\n",
              "<tr><td>H2O_cluster_status:</td>\n",
              "<td>locked, healthy</td></tr>\n",
              "<tr><td>H2O_connection_url:</td>\n",
              "<td>http://localhost:54321</td></tr>\n",
              "<tr><td>H2O_connection_proxy:</td>\n",
              "<td>{\"http\": null, \"https\": null, \"colab_language_server\": \"/usr/colab/bin/language_service\"}</td></tr>\n",
              "<tr><td>H2O_internal_security:</td>\n",
              "<td>False</td></tr>\n",
              "<tr><td>Python_version:</td>\n",
              "<td>3.12.12 final</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{\"user_id\":\"U9001\",\"txn_id\":10,\"event_time\":\"2026-03-03T01:13:00\",\"event_type\":\"withdrawal\",\"amount\":18000,\"currency\":\"usd\",\"channel\":\"mobile\",\"declared_income\":4000,\"account_deposit\":-17000,\"residence_country\":\"us\",\"geo_country\":\"sg\",\"amount_in_1d\":1000,\"amount_out_1d\":43000,\"login_count_1h\":18,\"failed_login_1h\":12,\"new_ip_1d\":true,\"geo_change_1d\":true}]\n",
            "Parse progress: |████████████████████████████████████████████████████████████████| (done) 100%\n",
            "extendedisolationforest prediction progress: |███████████████████████████████████| (done) 100%\n",
            "contributions progress: |"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/h2o/frame.py:1983: H2ODependencyWarning: Converting H2O frame to pandas dataframe using single-thread.  For faster conversion using multi-thread, install polars and pyarrow and use it as pandas_df = h2o_df.as_data_frame(use_multi_thread=True)\n",
            "\n",
            "  warnings.warn(\"Converting H2O frame to pandas dataframe using single-thread.  For faster conversion using\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "████████████████████████████████████████████████████████| (done) 100%\n",
            "Parse progress: |"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/h2o/frame.py:1983: H2ODependencyWarning: Converting H2O frame to pandas dataframe using single-thread.  For faster conversion using multi-thread, install polars and pyarrow and use it as pandas_df = h2o_df.as_data_frame(use_multi_thread=True)\n",
            "\n",
            "  warnings.warn(\"Converting H2O frame to pandas dataframe using single-thread.  For faster conversion using\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "████████████████████████████████████████████████████████████████| (done) 100%\n",
            "contributions progress: |████████████████████████████████████████████████████████| (done) 100%\n",
            "{\n",
            "  \"event_time\": \"2026-03-03 01:13:00\",\n",
            "  \"txn_id\": \"10\",\n",
            "  \"user_id\": \"U9001\",\n",
            "  \"is_anomaly\": 1,\n",
            "  \"detector_type\": \"BEHAVIOR\",\n",
            "  \"signal\": \"Liquidity Shift\",\n",
            "  \"severity\": \"HIGH\",\n",
            "  \"confidence\": \"0.991115070930257\",\n",
            "  \"evidence\": [\n",
            "    {\n",
            "      \"risk_category\": \"Liquidity Shift\",\n",
            "      \"feature\": \"net_flow_1d\",\n",
            "      \"impact\": \"increased\",\n",
            "      \"contribution\": 0.1174345463514328,\n",
            "      \"explanation\": \"Daily net cash flow shift detected (net_flow_1d=-42000.00). This increased anomaly risk.\"\n",
            "    },\n",
            "    {\n",
            "      \"risk_category\": \"Monetary Deviation\",\n",
            "      \"feature\": \"ewma_resid\",\n",
            "      \"impact\": \"reduced\",\n",
            "      \"contribution\": -0.006088281981647,\n",
            "      \"explanation\": \"Recent transaction amount differs from short-term trend (EWMA residual=0.00). This reduced anomaly risk.\"\n",
            "    }\n",
            "  ]\n",
            "}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/h2o/frame.py:1983: H2ODependencyWarning: Converting H2O frame to pandas dataframe using single-thread.  For faster conversion using multi-thread, install polars and pyarrow and use it as pandas_df = h2o_df.as_data_frame(use_multi_thread=True)\n",
            "\n",
            "  warnings.warn(\"Converting H2O frame to pandas dataframe using single-thread.  For faster conversion using\"\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "# 1. Load the model (Imagine this is a fresh server)\n",
        "prod_engine = BehaviorInferenceEngine()\n",
        "prod_engine.load_assets()\n",
        "\n",
        "# 2. Input JSON\n",
        "test_data_path = \"test_transactions 2.csv\"\n",
        "df_test = pd.read_csv(test_data_path)\n",
        "\n",
        "# raw_input = df_test[df_test['user_id'] == 'U1001'].to_json(orient='records')\n",
        "raw_input = df_test.iloc[9:10].to_json(orient='records')\n",
        "print(raw_input)\n",
        "\n",
        "# 3. Predict!\n",
        "result_json = prod_engine.predict(raw_input)\n",
        "print(json.dumps(result_json, indent=2))"
      ]
    }
  ]
}