{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM8E+ct5UDfeZLQXuOkpt5w",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ACTP2002/EVIDENCE/blob/behavior_model/test_model_v2_json_input.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --no-cache-dir -U numpy pandas scipy scikit-learn joblib tensorflow shap h2o"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rv-ePq2q2I_l",
        "outputId": "f27b6cb5-d26a-4b01-8cef-dd7fbb0c6952"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Collecting numpy\n",
            "  Downloading numpy-2.4.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Collecting pandas\n",
            "  Downloading pandas-3.0.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (1.16.3)\n",
            "Collecting scipy\n",
            "  Downloading scipy-1.17.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m198.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.8.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (1.5.3)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: shap in /usr/local/lib/python3.12/dist-packages (0.50.0)\n",
            "Collecting h2o\n",
            "  Downloading h2o-3.46.0.9-py2.py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: threadpoolctl>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.12.19)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.7.0)\n",
            "Requirement already satisfied: google_pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt_einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (26.0)\n",
            "Requirement already satisfied: protobuf>=5.28.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: typing_extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.1.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.76.0)\n",
            "Collecting tensorboard~=2.20.0 (from tensorflow)\n",
            "  Downloading tensorboard-2.20.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: keras>=3.10.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.15.1)\n",
            "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.12/dist-packages (from shap) (4.67.3)\n",
            "Requirement already satisfied: slicer==0.0.8 in /usr/local/lib/python3.12/dist-packages (from shap) (0.0.8)\n",
            "Requirement already satisfied: numba>=0.54 in /usr/local/lib/python3.12/dist-packages (from shap) (0.60.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.12/dist-packages (from shap) (3.1.2)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.12/dist-packages (from h2o) (0.9.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.46.3)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.10.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.10.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.10.0->tensorflow) (0.18.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.54->shap) (0.43.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2026.1.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.20.0->tensorflow) (3.10.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.20.0->tensorflow) (11.3.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.20.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.20.0->tensorflow) (3.1.5)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.10.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.10.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow) (0.1.2)\n",
            "Downloading pandas-3.0.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (10.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m208.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.17.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (35.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.0/35.0 MB\u001b[0m \u001b[31m178.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_learn-1.8.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (8.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m236.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow-2.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (620.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m620.7/620.7 MB\u001b[0m \u001b[31m249.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h2o-3.46.0.9-py2.py3-none-any.whl (266.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.0/266.0 MB\u001b[0m \u001b[31m164.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.20.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m179.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scipy, tensorboard, scikit-learn, pandas, h2o, tensorflow\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.16.3\n",
            "    Uninstalling scipy-1.16.3:\n",
            "      Successfully uninstalled scipy-1.16.3\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.19.0\n",
            "    Uninstalling tensorboard-2.19.0:\n",
            "      Successfully uninstalled tensorboard-2.19.0\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.6.1\n",
            "    Uninstalling scikit-learn-1.6.1:\n",
            "      Successfully uninstalled scikit-learn-1.6.1\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.19.0\n",
            "    Uninstalling tensorflow-2.19.0:\n",
            "      Successfully uninstalled tensorflow-2.19.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 3.0.0 which is incompatible.\n",
            "bqplot 0.12.45 requires pandas<3.0.0,>=1.0.0, but you have pandas 3.0.0 which is incompatible.\n",
            "gradio 5.50.0 requires pandas<3.0,>=1.0, but you have pandas 3.0.0 which is incompatible.\n",
            "db-dtypes 1.5.0 requires pandas<3.0.0,>=1.5.3, but you have pandas 3.0.0 which is incompatible.\n",
            "tensorflow-decision-forests 1.12.0 requires tensorflow==2.19.0, but you have tensorflow 2.20.0 which is incompatible.\n",
            "tf-keras 2.19.0 requires tensorflow<2.20,>=2.19, but you have tensorflow 2.20.0 which is incompatible.\n",
            "tensorflow-text 2.19.0 requires tensorflow<2.20,>=2.19.0, but you have tensorflow 2.20.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed h2o-3.46.0.9 pandas-3.0.0 scikit-learn-1.8.0 scipy-1.17.0 tensorboard-2.20.0 tensorflow-2.20.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.stats import median_abs_deviation\n",
        "import random\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "import h2o\n",
        "from h2o.estimators import H2OExtendedIsolationForestEstimator\n",
        "from h2o.estimators import H2OPrincipalComponentAnalysisEstimator\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, callbacks\n",
        "\n",
        "import shap\n",
        "import joblib\n",
        "\n",
        "import os\n",
        "import json\n"
      ],
      "metadata": {
        "id": "5pjmOL-a14_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BehaviorInferenceEngine:\n",
        "  RISK_MAPPING = {\n",
        "\n",
        "    # Monetary Behavior\n",
        "    \"mod_z_score_abs\": {\n",
        "        \"category\": \"Monetary Deviation\",\n",
        "        \"template\": \"Transaction amount deviates from user's historical behavior.\"\n",
        "    },\n",
        "    \"ewma_resid\": {\n",
        "        \"category\": \"Monetary Deviation\",\n",
        "        \"template\": \"Transaction differs from recent spending trend.\"\n",
        "    },\n",
        "    \"net_flow_1d\": {\n",
        "        \"category\": \"Liquidity Shift\",\n",
        "        \"template\": \"Unusual daily net cash flow movement detected.\"\n",
        "    },\n",
        "\n",
        "    # Temporal Behavior\n",
        "    \"gap_log\": {\n",
        "        \"category\": \"Temporal Anomaly\",\n",
        "        \"template\": \"Transaction timing gap is inconsistent with prior activity.\"\n",
        "    },\n",
        "\n",
        "    # Access Risk\n",
        "    \"login_count_1h\": {\n",
        "        \"category\": \"Access Risk\",\n",
        "        \"template\": \"Abnormal login frequency observed.\"\n",
        "    },\n",
        "    \"failed_login_ratio_1h\": {\n",
        "        \"category\": \"Access Risk\",\n",
        "        \"template\": \"Elevated failed login attempts detected.\"\n",
        "    },\n",
        "    \"new_ip_1d\": {\n",
        "        \"category\": \"Access Risk\",\n",
        "        \"template\": \"Transaction initiated from a new IP address.\"\n",
        "    },\n",
        "\n",
        "    # Geographic Risk\n",
        "    \"is_cross_border\": {\n",
        "        \"category\": \"Geolocation Risk\",\n",
        "        \"template\": \"Transaction occurred outside user's residence country.\"\n",
        "    }\n",
        "}\n",
        "\n",
        "  def __init__(self, model_dir=\"behavior_assets\"):\n",
        "    self.model_dir = model_dir\n",
        "    self.cat_features = [\"currency\", \"channel\", \"event_type\", \"geo_country\"]\n",
        "    self.num_features = [\"mod_z_score_abs\", \"ewma_resid\", \"gap_log\", \"net_flow_1d\", \"login_count_1h\", \"failed_login_ratio_1h\", \"new_ip_1d\", \"is_cross_border\"]\n",
        "    self.lstm_features = [\"amount_abs\", \"gap_log\", \"amount_to_income_ratio\", \"net_flow_1d\", \"deposit_to_income_ratio\", \"mod_z_score_abs\", \"ewma_resid\"]\n",
        "\n",
        "  # HELPER FUNCTIONS FOR SHAP\n",
        "  def _classify_severity(self, score, is_anomaly):\n",
        "      if score < 0.30 or not is_anomaly:\n",
        "          return \"LOW\"\n",
        "      elif score < 0.70:\n",
        "          return \"MEDIUM\"\n",
        "      else:\n",
        "          return \"HIGH\"\n",
        "\n",
        "  def _apply_confidence(self, df, threshold, std):\n",
        "      k = 3.0 / (std + 1e-9) # Using 3.0 makes the curve slightly smoother\n",
        "\n",
        "      # Calculate distance from threshold\n",
        "      # If positive, it's an anomaly; if negative, it's normal\n",
        "      diff = df[\"final_score\"] - threshold\n",
        "\n",
        "      # Apply Sigmoid\n",
        "      # This maps scores:\n",
        "      # Much higher than threshold -> ~1.0\n",
        "      # Exactly threshold -> 0.5\n",
        "      # Much lower than threshold -> ~0.0 (meaning 100% confident it is NORMAL)\n",
        "      conf = 1 / (1 + np.exp(-k * diff))\n",
        "\n",
        "      # Confidence in the decision (0.5 to 1)\n",
        "      return np.where(conf >= 0.5, conf, 1 - conf)\n",
        "\n",
        "  def _generate_human_explanation(self, feature, value, shap_value):\n",
        "\n",
        "      direction = \"increased\" if shap_value > 0 else \"reduced\"\n",
        "\n",
        "      def safe_float(v):\n",
        "          try:\n",
        "              return float(v)\n",
        "          except:\n",
        "              return None\n",
        "\n",
        "      value = safe_float(value)\n",
        "\n",
        "      explanations = {\n",
        "          \"mod_z_score_abs\": (\n",
        "              f\"Transaction amount deviates significantly from user's normal behavior \"\n",
        "              f\"(Z-score={value:.2f}). This {direction} anomaly risk.\"\n",
        "              if value is not None else\n",
        "              f\"Transaction amount deviates significantly from user's normal behavior. \"\n",
        "              f\"This {direction} anomaly risk.\"\n",
        "          ),\n",
        "\n",
        "          \"ewma_resid\": (\n",
        "              f\"Recent transaction amount differs from short-term trend \"\n",
        "              f\"(EWMA residual={value:.2f}). This {direction} anomaly risk.\"\n",
        "              if value is not None else\n",
        "              f\"Recent transaction amount differs from short-term trend. \"\n",
        "              f\"This {direction} anomaly risk.\"\n",
        "          ),\n",
        "\n",
        "          \"gap_log\": (\n",
        "              f\"Transaction timing gap is unusual compared to prior activity \"\n",
        "              f\"(gap_log={value:.2f}). This {direction} anomaly risk.\"\n",
        "              if value is not None else\n",
        "              f\"Transaction timing gap is unusual compared to prior activity. \"\n",
        "              f\"This {direction} anomaly risk.\"\n",
        "          ),\n",
        "\n",
        "          \"net_flow_1d\": (\n",
        "              f\"Daily net cash flow shift detected \"\n",
        "              f\"(net_flow_1d={value:.2f}). This {direction} anomaly risk.\"\n",
        "              if value is not None else\n",
        "              f\"Daily net cash flow shift detected. \"\n",
        "              f\"This {direction} anomaly risk.\"\n",
        "          ),\n",
        "\n",
        "          \"login_count_1h\": (\n",
        "              f\"Abnormal login frequency in past hour \"\n",
        "              f\"(count={int(value)}). This {direction} anomaly risk.\"\n",
        "              if value is not None else\n",
        "              f\"Abnormal login frequency in past hour. \"\n",
        "              f\"This {direction} anomaly risk.\"\n",
        "          ),\n",
        "\n",
        "          \"failed_login_ratio_1h\": (\n",
        "              f\"Elevated failed login attempts ratio \"\n",
        "              f\"({value:.2f}). This {direction} anomaly risk.\"\n",
        "              if value is not None else\n",
        "              f\"Elevated failed login attempts detected. \"\n",
        "              f\"This {direction} anomaly risk.\"\n",
        "          ),\n",
        "\n",
        "          \"new_ip_1d\":\n",
        "              f\"New IP address detected in last 24h. \"\n",
        "              f\"This {direction} anomaly risk.\",\n",
        "\n",
        "          \"is_cross_border\":\n",
        "              f\"Transaction occurred outside user's residence country. \"\n",
        "              f\"This {direction} anomaly risk.\"\n",
        "      }\n",
        "\n",
        "      return explanations.get(\n",
        "          feature,\n",
        "          f\"{feature} contributed to anomaly score and {direction} risk.\"\n",
        "      )\n",
        "\n",
        "  def _compute_shap_batch(self, df_input):\n",
        "\n",
        "      if isinstance(df_input, dict):\n",
        "          df_input = pd.DataFrame([df_input])\n",
        "\n",
        "      h2o_frame = h2o.H2OFrame(df_input)\n",
        "\n",
        "      for col in self.cat_features:\n",
        "          if col in df_input.columns:\n",
        "              h2o_frame[col] = h2o_frame[col].asfactor()\n",
        "\n",
        "      shap_val = self.surrogate.predict_contributions(h2o_frame)\n",
        "      shap_df = shap_val.as_data_frame().drop(columns=[\"BiasTerm\"])\n",
        "\n",
        "      return shap_df\n",
        "\n",
        "  def _convert_to_risk_evidence(self, feature, shap_value, raw_value):\n",
        "\n",
        "      if feature not in self.RISK_MAPPING:\n",
        "          return None\n",
        "\n",
        "      risk_info = self.RISK_MAPPING[feature]\n",
        "      direction = \"increased\" if shap_value > 0 else \"reduced\"\n",
        "\n",
        "      explanation_text = self._generate_human_explanation(\n",
        "          feature, raw_value, shap_value\n",
        "      )\n",
        "\n",
        "      return {\n",
        "          \"risk_category\": risk_info[\"category\"],\n",
        "          \"feature\": feature,\n",
        "          \"impact\": direction,\n",
        "          \"contribution\": float(shap_value),\n",
        "          \"explanation\": explanation_text\n",
        "      }\n",
        "\n",
        "\n",
        "  def _build_behavior_output(self, df_input, top_n=2, min_abs_contribution=1e-4):\n",
        "\n",
        "      if isinstance(df_input, dict):\n",
        "          df_input = pd.DataFrame([df_input])\n",
        "\n",
        "      shap_df = self._compute_shap_batch(df_input)\n",
        "\n",
        "      results = []\n",
        "\n",
        "      for idx in range(len(df_input)):\n",
        "\n",
        "          row = df_input.iloc[idx]\n",
        "          shap_series = shap_df.iloc[idx]\n",
        "\n",
        "          # Remove tiny noise\n",
        "          shap_series = shap_series[shap_series.abs() > min_abs_contribution]\n",
        "\n",
        "          # Sort by absolute contribution\n",
        "          shap_series = shap_series.reindex(\n",
        "              shap_series.abs().sort_values(ascending=False).index\n",
        "          )\n",
        "\n",
        "          # Top drivers\n",
        "          top_features = shap_series.head(top_n)\n",
        "\n",
        "          evidence_list = []\n",
        "\n",
        "          for feature, shap_value in top_features.items():\n",
        "\n",
        "              raw_value = row.get(feature, None)\n",
        "\n",
        "              evidence = self._convert_to_risk_evidence(\n",
        "                  feature, shap_value, raw_value\n",
        "              )\n",
        "\n",
        "              if evidence:\n",
        "                  evidence_list.append(evidence)\n",
        "\n",
        "          # -------- Highest impact signal --------\n",
        "          if evidence_list:\n",
        "              highest_signal = evidence_list[0][\"risk_category\"]\n",
        "          else:\n",
        "              highest_signal = \"No significant anomaly drivers detected.\"\n",
        "\n",
        "          # -------- Severity --------\n",
        "          anomaly_level = float(row[\"final_score\"]/self.threshold)\n",
        "          severity = self._classify_severity(anomaly_level,row[\"is_anomaly\"])\n",
        "\n",
        "          # -------- Output --------\n",
        "\n",
        "          result = {\n",
        "              \"event_time\": str(row[\"event_time\"]),\n",
        "              \"txn_id\": str(row[\"txn_id\"]),\n",
        "              \"user_id\": str(row[\"user_id\"]),\n",
        "              \"is_anomaly\": int(row[\"is_anomaly\"]),\n",
        "              \"detector_type\": \"BEHAVIOR\",\n",
        "              \"signal\": highest_signal,\n",
        "              \"severity\": severity,\n",
        "              \"confidence\": str(row[\"confidence_score\"]),\n",
        "              \"evidence\": evidence_list\n",
        "          }\n",
        "\n",
        "          results.append(result)\n",
        "\n",
        "      if len(results) == 1:\n",
        "          return results[0]\n",
        "\n",
        "      return results\n",
        "\n",
        "  def save_assets(self, eif, surrogate, lstm, seq_scaler, feature_scaler, cohort_stats, global_stats, threshold, weights, scores_std):\n",
        "    if not os.path.exists(self.model_dir): os.makedirs(self.model_dir)\n",
        "    h2o.save_model(model=eif, path=self.model_dir, force=True)\n",
        "    h2o.save_model(model=surrogate, path=self.model_dir, force=True)\n",
        "    lstm.save(f\"{self.model_dir}/lstm_model.h5\")\n",
        "    joblib.dump(seq_scaler, f\"{self.model_dir}/seq_scaler.pkl\")\n",
        "    joblib.dump(feature_scaler, f\"{self.model_dir}/feature_scaler.pkl\")\n",
        "    joblib.dump(cohort_stats, f\"{self.model_dir}/cohort_stats.pkl\")\n",
        "    joblib.dump(global_stats, f\"{self.model_dir}/global_stats.pkl\")\n",
        "    joblib.dump({\"threshold\": threshold, \"weights\": weights, \"eif_id\": eif.model_id, \"surr_id\": surrogate.model_id, \"scores_std\": scores_std}, f\"{self.model_dir}/meta.pkl\")\n",
        "\n",
        "  def load_assets(self):\n",
        "    h2o.init()\n",
        "    meta = joblib.load(f\"{self.model_dir}/meta.pkl\")\n",
        "    self.threshold = meta['threshold']\n",
        "    self.weights = meta['weights']\n",
        "    self.scores_std = meta['scores_std']\n",
        "    self.eif = h2o.load_model(f\"{self.model_dir}/{meta['eif_id']}\")\n",
        "    self.surrogate = h2o.load_model(f\"{self.model_dir}/{meta['surr_id']}\")\n",
        "    self.lstm = tf.keras.models.load_model(f\"{self.model_dir}/lstm_model.h5\", custom_objects={\"mse\": tf.keras.losses.MeanSquaredError()})\n",
        "    self.seq_scaler = joblib.load(f\"{self.model_dir}/seq_scaler.pkl\")\n",
        "    self.feature_scaler = joblib.load(f\"{self.model_dir}/feature_scaler.pkl\")\n",
        "    self.cohort_stats = joblib.load(f\"{self.model_dir}/cohort_stats.pkl\")\n",
        "    self.global_stats = joblib.load(f\"{self.model_dir}/global_stats.pkl\")\n",
        "\n",
        "  def _engineer(self, df):\n",
        "    df = df.copy()\n",
        "    df[\"event_time\"] = pd.to_datetime(df[\"event_time\"])\n",
        "    df = df.sort_values(\"event_time\")\n",
        "    for c in [\"currency\", \"channel\", \"residence_country\", \"geo_country\", \"event_type\"]:\n",
        "        if c in df.columns: df[c] = df[c].astype(str).str.strip().str.lower()\n",
        "\n",
        "    # Financial ratios calculation\n",
        "    df[\"amount_abs\"] = df[\"amount\"].abs()\n",
        "    df[\"amount_to_income_ratio\"] = df[\"amount_abs\"] / (df[\"declared_income\"] + 1e-9)\n",
        "    df[\"deposit_to_income_ratio\"] = df[\"account_deposit\"] / (df[\"declared_income\"] + 1e-9)\n",
        "    df[\"net_flow_1d\"] = df[\"amount_in_1d\"] - df[\"amount_out_1d\"]\n",
        "\n",
        "    # Location\n",
        "    df[\"is_cross_border\"] = (df[\"residence_country\"] != df[\"geo_country\"]).astype(int)\n",
        "    df[\"failed_login_ratio_1h\"] = df[\"failed_login_1h\"] / (df[\"login_count_1h\"] + 1e-9)\n",
        "    df[\"new_ip_1d\"] = df[\"new_ip_1d\"].fillna(0)\n",
        "    df[\"geo_change_1d\"] = df[\"geo_change_1d\"].fillna(0)\n",
        "\n",
        "    # Transaction gap\n",
        "    df[\"gap_seconds\"] = df[\"event_time\"].diff().dt.total_seconds().fillna(self.global_stats['median_gap'])\n",
        "    df[\"gap_log\"] = np.log1p(df[\"gap_seconds\"])\n",
        "\n",
        "    # Window period\n",
        "    df[\"user_median_15\"] = df[\"amount_abs\"].rolling(window=15, min_periods=1).median()\n",
        "    df[\"user_mad_15\"] = df[\"amount_abs\"].rolling(window=15, min_periods=1).apply(lambda x: median_abs_deviation(x, scale='normal') if len(x)>1 else self.global_stats['mad'], raw=False)\n",
        "\n",
        "    for col in [\"cohort_median\", \"cohort_mad\"]:\n",
        "      if col in df.columns:\n",
        "          df = df.drop(columns=col)\n",
        "\n",
        "    df = df.merge(self.cohort_stats, on=[\"currency\", \"geo_country\", \"channel\", \"event_type\"], how=\"left\")\n",
        "    b_med = df[\"user_median_15\"].fillna(df[\"cohort_median\"]).fillna(self.global_stats['median_amt'])\n",
        "    b_mad = df[\"user_mad_15\"].fillna(df[\"cohort_mad\"]).fillna(self.global_stats['mad'])\n",
        "    df[\"mod_z_score_abs\"] = (0.6745 * (df[\"amount_abs\"] - b_med) / (b_mad + 1e-9)).abs()\n",
        "    df[\"ewma_resid\"] = (df[\"amount_abs\"] - df[\"amount_abs\"].ewm(span=8).mean()).abs()\n",
        "    return df\n",
        "\n",
        "  def predict(self, raw_json):\n",
        "    # 1. Parse & Engineer\n",
        "    input_df = pd.DataFrame(json.loads(raw_json))\n",
        "    feat_df = self._engineer(input_df)\n",
        "\n",
        "    # 2. Points Score (EIF)\n",
        "    h2o_fr = h2o.H2OFrame(feat_df[self.cat_features + self.num_features])\n",
        "    for c in self.cat_features: h2o_fr[c] = h2o_fr[c].asfactor()\n",
        "    feat_df[\"iforest_score\"] = self.eif.predict(h2o_fr)[\"anomaly_score\"].as_data_frame().iloc[:, 0].values\n",
        "\n",
        "    # 3. Sequence Score (LSTM)\n",
        "    scaled_lstm = self.seq_scaler.transform(feat_df[self.lstm_features].fillna(0))\n",
        "    if len(scaled_lstm) >= 20:\n",
        "        seq = np.array([scaled_lstm[-20:]])\n",
        "        feat_df.loc[feat_df.index[-1], \"lstm_score\"] = np.mean((seq - self.lstm.predict(seq, verbose=0))**2)\n",
        "    else:\n",
        "        feat_df[\"lstm_score\"] = self.global_stats['lstm_median']\n",
        "\n",
        "    # 4. Ensemble\n",
        "    detector_cols = [\"mod_z_score_abs\", \"ewma_resid\", \"iforest_score\", \"lstm_score\"]\n",
        "    n_vals = self.feature_scaler.transform(feat_df[detector_cols].fillna(0))\n",
        "    feat_df[\"final_score\"] = np.sum([self.weights[c] * n_vals[:, i] for i, c in enumerate(detector_cols)], axis=0)\n",
        "    feat_df[\"is_anomaly\"] = (feat_df[\"final_score\"] >= self.threshold).astype(int)\n",
        "    feat_df[\"confidence_score\"] = self._apply_confidence(feat_df, self.threshold, self.scores_std)\n",
        "\n",
        "    # 5. SHAP & Formatting\n",
        "    final_prediction = self._build_behavior_output(feat_df)\n",
        "\n",
        "    return final_prediction\n"
      ],
      "metadata": {
        "id": "4jxqlI7t1veN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "\n",
        "class EventMerger:\n",
        "\n",
        "    def __init__(self, auth_file, profile_file, status_file, txn_file):\n",
        "        self.auth = self._load_json(auth_file)\n",
        "        self.profile = self._load_json(profile_file)\n",
        "        self.status = self._load_json(status_file)\n",
        "        self.txn = self._load_json(txn_file)\n",
        "\n",
        "    @staticmethod\n",
        "    def _load_json(file_path):\n",
        "        with open(file_path, \"r\") as f:\n",
        "            return json.load(f)\n",
        "\n",
        "    @staticmethod\n",
        "    def _safe_number(value):\n",
        "        if value is None:\n",
        "            return None\n",
        "        try:\n",
        "            return float(value)\n",
        "        except (ValueError, TypeError):\n",
        "            return None\n",
        "\n",
        "    def merge(self):\n",
        "        user_id = self.txn.get(\"user_id\")\n",
        "\n",
        "        output = {\n",
        "            \"user_id\": user_id,\n",
        "\n",
        "            # Transaction fields\n",
        "            \"txn_id\": self.txn.get(\"data\", {}).get(\"txn_id\"),\n",
        "            \"event_time\": self.txn.get(\"event_time\"),\n",
        "            \"event_type\": self.txn.get(\"event_type\"),\n",
        "            \"amount\": self.txn.get(\"data\", {}).get(\"amount\"),\n",
        "            \"currency\": str(self.txn.get(\"data\", {}).get(\"currency\", \"\")).lower(),\n",
        "            \"channel\": self.txn.get(\"data\", {}).get(\"channel\"),\n",
        "\n",
        "            # Profile fields\n",
        "            \"declared_income\": self._safe_number(\n",
        "                self.profile.get(\"kyc\", {}).get(\"income\")\n",
        "            ),\n",
        "            \"account_deposit\": self._safe_number(\n",
        "                self.profile.get(\"account\", {}).get(\"account_deposit\")\n",
        "            ),\n",
        "            \"residence_country\": str(\n",
        "                self.profile.get(\"kyc\", {}).get(\"residence_country\", \"\")\n",
        "            ).lower(),\n",
        "\n",
        "            # Auth / Geo fields\n",
        "            \"geo_country\": str(\n",
        "                self.auth.get(\"data\", {}).get(\"geo\", {}).get(\"country\", \"\")\n",
        "            ).lower(),\n",
        "\n",
        "            # Status - Transaction Metrics\n",
        "            \"amount_in_1d\": self.status.get(\"txn\", {}).get(\"amount_in_1d\"),\n",
        "            \"amount_out_1d\": self.status.get(\"txn\", {}).get(\"amount_out_1d\"),\n",
        "\n",
        "            # Status - Auth Metrics\n",
        "            \"login_count_1h\": self.status.get(\"auth\", {}).get(\"login_count_1h\"),\n",
        "            \"failed_login_1h\": self.status.get(\"auth\", {}).get(\"failed_login_1h\"),\n",
        "            \"new_ip_1d\": bool(self.status.get(\"auth\", {}).get(\"new_ip_1d\")),\n",
        "            \"geo_change_1d\": bool(self.status.get(\"network\", {}).get(\"geo_change_1d\"))\n",
        "        }\n",
        "        output_format = [output]\n",
        "        return json.dumps(output_format, indent=4)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7GvAESmS2_S0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KKOMC5hv1RIW",
        "outputId": "201d0c1b-0da7-4d40-f37f-d93a4fab742c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking whether there is an H2O instance running at http://localhost:54321. connected.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "--------------------------  -----------------------------------------------------------------------------------------\n",
              "H2O_cluster_uptime:         17 mins 31 secs\n",
              "H2O_cluster_timezone:       Etc/UTC\n",
              "H2O_data_parsing_timezone:  UTC\n",
              "H2O_cluster_version:        3.46.0.9\n",
              "H2O_cluster_version_age:    2 months and 20 days\n",
              "H2O_cluster_name:           H2O_from_python_unknownUser_saamqs\n",
              "H2O_cluster_total_nodes:    1\n",
              "H2O_cluster_free_memory:    3.100 Gb\n",
              "H2O_cluster_total_cores:    2\n",
              "H2O_cluster_allowed_cores:  2\n",
              "H2O_cluster_status:         locked, healthy\n",
              "H2O_connection_url:         http://localhost:54321\n",
              "H2O_connection_proxy:       {\"http\": null, \"https\": null, \"colab_language_server\": \"/usr/colab/bin/language_service\"}\n",
              "H2O_internal_security:      False\n",
              "Python_version:             3.12.12 final\n",
              "--------------------------  -----------------------------------------------------------------------------------------"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "\n",
              "#h2o-table-14.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-14 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-14 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-14 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-14 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-14 .h2o-table th,\n",
              "#h2o-table-14 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-14 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-14\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption></caption>\n",
              "    <thead></thead>\n",
              "    <tbody><tr><td>H2O_cluster_uptime:</td>\n",
              "<td>17 mins 31 secs</td></tr>\n",
              "<tr><td>H2O_cluster_timezone:</td>\n",
              "<td>Etc/UTC</td></tr>\n",
              "<tr><td>H2O_data_parsing_timezone:</td>\n",
              "<td>UTC</td></tr>\n",
              "<tr><td>H2O_cluster_version:</td>\n",
              "<td>3.46.0.9</td></tr>\n",
              "<tr><td>H2O_cluster_version_age:</td>\n",
              "<td>2 months and 20 days</td></tr>\n",
              "<tr><td>H2O_cluster_name:</td>\n",
              "<td>H2O_from_python_unknownUser_saamqs</td></tr>\n",
              "<tr><td>H2O_cluster_total_nodes:</td>\n",
              "<td>1</td></tr>\n",
              "<tr><td>H2O_cluster_free_memory:</td>\n",
              "<td>3.100 Gb</td></tr>\n",
              "<tr><td>H2O_cluster_total_cores:</td>\n",
              "<td>2</td></tr>\n",
              "<tr><td>H2O_cluster_allowed_cores:</td>\n",
              "<td>2</td></tr>\n",
              "<tr><td>H2O_cluster_status:</td>\n",
              "<td>locked, healthy</td></tr>\n",
              "<tr><td>H2O_connection_url:</td>\n",
              "<td>http://localhost:54321</td></tr>\n",
              "<tr><td>H2O_connection_proxy:</td>\n",
              "<td>{\"http\": null, \"https\": null, \"colab_language_server\": \"/usr/colab/bin/language_service\"}</td></tr>\n",
              "<tr><td>H2O_internal_security:</td>\n",
              "<td>False</td></tr>\n",
              "<tr><td>Python_version:</td>\n",
              "<td>3.12.12 final</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\n",
            "    {\n",
            "        \"user_id\": \"u_001\",\n",
            "        \"txn_id\": \"T-883192\",\n",
            "        \"event_time\": \"2026-02-07T09:15:12.345Z\",\n",
            "        \"event_type\": \"buy\",\n",
            "        \"amount\": 1083.21,\n",
            "        \"currency\": \"usd\",\n",
            "        \"channel\": \"web\",\n",
            "        \"declared_income\": 5000.0,\n",
            "        \"account_deposit\": 52000.0,\n",
            "        \"residence_country\": \"my\",\n",
            "        \"geo_country\": \"my\",\n",
            "        \"amount_in_1d\": 0,\n",
            "        \"amount_out_1d\": 50000,\n",
            "        \"login_count_1h\": 1,\n",
            "        \"failed_login_1h\": 0,\n",
            "        \"new_ip_1d\": true,\n",
            "        \"geo_change_1d\": false\n",
            "    }\n",
            "]\n",
            "Parse progress: |████████████████████████████████████████████████████████████████| (done) 100%\n",
            "extendedisolationforest prediction progress: |███████████████████████████████████| (done) 100%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/h2o/frame.py:1983: H2ODependencyWarning: Converting H2O frame to pandas dataframe using single-thread.  For faster conversion using multi-thread, install polars and pyarrow and use it as pandas_df = h2o_df.as_data_frame(use_multi_thread=True)\n",
            "\n",
            "  warnings.warn(\"Converting H2O frame to pandas dataframe using single-thread.  For faster conversion using\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parse progress: |████████████████████████████████████████████████████████████████| (done) 100%\n",
            "contributions progress: |████████████████████████████████████████████████████████| (done) 100%\n",
            "{\n",
            "  \"event_time\": \"2026-02-07 09:15:12.345000+00:00\",\n",
            "  \"txn_id\": \"T-883192\",\n",
            "  \"user_id\": \"u_001\",\n",
            "  \"is_anomaly\": 1,\n",
            "  \"detector_type\": \"BEHAVIOR\",\n",
            "  \"signal\": \"Liquidity Shift\",\n",
            "  \"severity\": \"HIGH\",\n",
            "  \"confidence\": \"0.986558766168058\",\n",
            "  \"evidence\": [\n",
            "    {\n",
            "      \"risk_category\": \"Liquidity Shift\",\n",
            "      \"feature\": \"net_flow_1d\",\n",
            "      \"impact\": \"increased\",\n",
            "      \"contribution\": 0.1242170557379722,\n",
            "      \"explanation\": \"Daily net cash flow shift detected (net_flow_1d=-50000.00). This increased anomaly risk.\"\n",
            "    },\n",
            "    {\n",
            "      \"risk_category\": \"Monetary Deviation\",\n",
            "      \"feature\": \"ewma_resid\",\n",
            "      \"impact\": \"reduced\",\n",
            "      \"contribution\": -0.0055617871694266,\n",
            "      \"explanation\": \"Recent transaction amount differs from short-term trend (EWMA residual=0.00). This reduced anomaly risk.\"\n",
            "    }\n",
            "  ]\n",
            "}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/h2o/frame.py:1983: H2ODependencyWarning: Converting H2O frame to pandas dataframe using single-thread.  For faster conversion using multi-thread, install polars and pyarrow and use it as pandas_df = h2o_df.as_data_frame(use_multi_thread=True)\n",
            "\n",
            "  warnings.warn(\"Converting H2O frame to pandas dataframe using single-thread.  For faster conversion using\"\n"
          ]
        }
      ],
      "source": [
        "# 1. Load the model (Imagine this is a fresh server)\n",
        "prod_engine = BehaviorInferenceEngine()\n",
        "prod_engine.load_assets()\n",
        "\n",
        "# 2. Input JSON\n",
        "# test_data_path = \"test_transactions 2.csv\"\n",
        "# df_test = pd.read_csv(test_data_path)\n",
        "merger = EventMerger(\"auth_sample.json\",\"profile_sample.json\",\"status_sample.json\",\"transaction_sample.json\")\n",
        "\n",
        "json_anomaly_test = merger.merge()\n",
        "\n",
        "# raw_input = df_test[df_test['user_id'] == 'U1001'].to_json(orient='records')\n",
        "# raw_input = df_test.iloc[9:10].to_json(orient='records')\n",
        "# print(raw_input)\n",
        "print(json_anomaly_test)\n",
        "\n",
        "# 3. Predict!\n",
        "result_json = prod_engine.predict(json_anomaly_test)\n",
        "print(json.dumps(result_json, indent=2))"
      ]
    }
  ]
}